{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How good is your model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Metrics**\n",
    "\n",
    "- Measuring model performance with accuracy:\n",
    "    - Fraction of correctly classified samples;\n",
    "    - Not always a useful metric;\n",
    "    \n",
    "**Class imbalance examples: Emails**\n",
    "\n",
    "- Spam classification: \n",
    "    - 99% of emails are real and only 1% are spam. \n",
    "- Could build a classifier that ALL emails as real:\n",
    "    - This model would be correct 99% of the time. 99% accurate!\n",
    "    - But horrible at actually classifying spam. \n",
    "    - It neves predicts spam at all, so it completely fails at its original purpose. \n",
    "\n",
    "The situation when one class is more frequent is called __class imbalance__, because the class of real emails contains way more instances than the class of spam. This is a very commom situation in pratice and requires a more nuanced metric to assess the perfomance of our model. \n",
    "\n",
    "**Diagnosing classification predictions**\n",
    "\n",
    "Given a binary classifier, such as our spam email example, we can draw up a 2-by-2 matrix that summarizes predictive perfomance called a __confusion matrix__:\n",
    "\n",
    "<img src=\"images/confusionmatrix.png\" width=\"400\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any model, we can fill in the confunsion matrix according to this predictions. \n",
    "- In the top left square (true positive), we have the number of spam emails correctly labeled;\n",
    "- In the bottom right square (true negative), we have the number of real emails correclty labeled; \n",
    "- In the top right, the number of spam emails incorrectly labeled;\n",
    "- In the bottom left, the number of real emails incorrectly labeled. \n",
    "\n",
    "Usually, the \"class of interest\" is called the positive class. As we are trying to detect spam, this makes spam the positive class. Which class you call positive is really up to you. So why do we care about the confusion matrix? \n",
    "\n",
    "- Accuracy (you can retrieve acc from the confusion matrix): \n",
    "    - It's the sum of the diagnoal divided by the total sum of the matrix.\n",
    "    \n",
    "        $\\frac{tp+tn}{tp+tn+fp+fn}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics from the Confusion Matrix**\n",
    "\n",
    "There are several other important metrics, that we can easily calculate from them confusion matrix.\n",
    "- Precision: which is the number of true positives divided by the total number of true positives and false positives. It's also called the positive predictive value or PPV. \n",
    "(In our example, this is the number of correctly labeled spam emails, divided by the total number of emails classified as spam). \n",
    "- Recall: which is the number of true positives divided by the total number of true positives and false negatives. This is also called sensitivity, hit rate, or true positive rate.\n",
    "- F1-score: is defined as two time the product of the precision and recall divided by the sum of the precision and recall, in other words, it's the harmonic mean of precision and recall. \n",
    "\n",
    "Precision: $\\frac{tp}{tp+fp}$ \n",
    "\n",
    "Recall: $\\frac{tp}{tp+fn}$\n",
    "\n",
    "F1score: $2*\\frac{precision*recall}{precision+recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix in scikit-learn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Instatite our classifier, split the data into train and test\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "#To compute the confusion matrix, we pass te test set labels \n",
    "#and the predicted labels to the function confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all metrics in scikit-learn, the firts arguments is always the true label and the prediction is always the second argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression and the ROC curve\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its name, logistic regression is used in classification problems, not regression problems. \n",
    "\n",
    "**Logistic regression for binary classification**\n",
    "- Given one feature, log reg will output a probability, _p_, with respect to the target variable. \n",
    "- If the probability _p_ is greater than 0.5: we label the data as **\"1\"**;\n",
    "- If the probability _p_ is less than 0.5: we label the data as **\"0\"**;\n",
    "\n",
    "\n",
    "<img src=\"images/lineardescisionboundary.png\" width=\"400\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
