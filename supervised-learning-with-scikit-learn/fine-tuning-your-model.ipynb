{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How good is your model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Metrics**\n",
    "\n",
    "- Measuring model performance with accuracy:\n",
    "    - Fraction of correctly classified samples;\n",
    "    - Not always a useful metric;\n",
    "    \n",
    "**Class imbalance examples: Emails**\n",
    "\n",
    "- Spam classification: \n",
    "    - 99% of emails are real and only 1% are spam. \n",
    "- Could build a classifier that ALL emails as real:\n",
    "    - This model would be correct 99% of the time. 99% accurate!\n",
    "    - But horrible at actually classifying spam. \n",
    "    - It neves predicts spam at all, so it completely fails at its original purpose. \n",
    "\n",
    "The situation when one class is more frequent is called __class imbalance__, because the class of real emails contains way more instances than the class of spam. This is a very commom situation in pratice and requires a more nuanced metric to assess the perfomance of our model. \n",
    "\n",
    "**Diagnosing classification predictions**\n",
    "\n",
    "Given a binary classifier, such as our spam email example, we can draw up a 2-by-2 matrix that summarizes predictive perfomance called a __confusion matrix__:\n",
    "\n",
    "<img src=\"images/confusionmatrix.png\" width=\"400\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any model, we can fill in the confunsion matrix according to this predictions. \n",
    "- In the top left square (true positive), we have the number of spam emails correctly labeled;\n",
    "- In the bottom right square (true negative), we have the number of real emails correclty labeled; \n",
    "- In the top right, the number of spam emails incorrectly labeled;\n",
    "- In the bottom left, the number of real emails incorrectly labeled. \n",
    "\n",
    "Usually, the \"class of interest\" is called the positive class. As we are trying to detect spam, this makes spam the positive class. Which class you call positive is really up to you. So why do we care about the confusion matrix? \n",
    "\n",
    "- Accuracy (you can retrieve acc from the confusion matrix): \n",
    "    - It's the sum of the diagnoal divided by the total sum of the matrix.\n",
    "    \n",
    "        $\\frac{tp+tn}{tp+tn+fp+fn}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics from the Confusion Matrix**\n",
    "\n",
    "There are several other important metrics, that we can easily calculate from them confusion matrix.\n",
    "- Precision: which is the number of true positives divided by the total number of true positives and false positives. It's also called the positive predictive value or PPV. \n",
    "(In our example, this is the number of correctly labeled spam emails, divided by the total number of emails classified as spam). \n",
    "- Recall: which is the number of true positives divided by the total number of true positives and false negatives. This is also called sensitivity, hit rate, or true positive rate.\n",
    "- F1-score: is defined as two time the product of the precision and recall divided by the sum of the precision and recall, in other words, it's the harmonic mean of precision and recall. \n",
    "\n",
    "Precision: $\\frac{tp}{tp+fp}$ \n",
    "\n",
    "Recall: $\\frac{tp}{tp+fn}$\n",
    "\n",
    "F1score: $2*\\frac{precision*recall}{precision+recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix in scikit-learn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Instatite our classifier, split the data into train and test\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "#To compute the confusion matrix, we pass te test set labels \n",
    "#and the predicted labels to the function confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all metrics in scikit-learn, the firts arguments is always the true label and the prediction is always the second argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression and the ROC curve\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its name, logistic regression is used in classification problems, not regression problems. \n",
    "\n",
    "**Logistic regression for binary classification**\n",
    "- Given one feature, log reg will output a probability, _p_, with respect to the target variable. \n",
    "- If the probability _p_ is greater than 0.5: we label the data as **\"1\"**;\n",
    "- If the probability _p_ is less than 0.5: we label the data as **\"0\"**;\n",
    "\n",
    "\n",
    "<img src=\"images/lineardescisionboundary.png\" width=\"400\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that log reg produces a linear decision boundary. Using logistic regression in scikit-learn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability thresholds**\n",
    "\n",
    "- Notice that in defining logistic regression (by default), we have specified a threshold of 0.5 for the probability, a threshold that defines our model. \n",
    "\n",
    "- This is not particular for log reg but also could be used for KNN, for example. \n",
    "\n",
    "- What happens if we vary the threshold? In particular, what happens to the true positive and false positive rates as we vary the threshold? \n",
    "\n",
    "When the threshold = \"0\" (p = 0), the model predicts \"1\" for all the data, which means the true positive rate is equal to the false positive rate, is equal to one.\n",
    "\n",
    "When the threshold = \"1\", the model predicts \"0\" for all data, which means that both true and false positive rates are \"0\".\n",
    "\n",
    "If we vary the threshold between these two extremes, we get a series of diferent false positive and true positive rates.\n",
    "\n",
    "<img src=\"images/theROCcurve.png\" width=\"400\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of points we get when trying all possible thresholds is called the **Receiver Operationg Characteristic** curve or **ROC** curve. \n",
    "\n",
    "To plot the ROC curve:\n",
    "- we impoprte roc curve from `sklearn.metrics`; \n",
    "- we then call the function roc curve;\n",
    "- the first argument is given by actual labelas;\n",
    "- the second by the predicted probabilites;\n",
    "- we unpack the result into three variables: false positive rate, FPR; true positive rate, TPR; and the thresholds. \n",
    "- we can then plot the fpr and tpr using pyplot's plot function to produce a figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0,1] , 'k--')\n",
    "plt.plot(frr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function produce a figure such as this:\n",
    "\n",
    "<img src=\"images/plotROCcurve.png\" width=\"400\" style=\"float:left\"/>\n",
    "\n",
    "\n",
    "We used the predicted probabilites of the model assigning a value of \"1\" to the observation in question. This is because to compute the ROC we don't merely want the predictions on the test set, but we want the probability that our log reg model outputs before using a threashold to predict the label. \n",
    "\n",
    "To do this we apply the method predict proba to the model and pass it the data. Predict proba returns an array with two columns: each column contains the probabilities for the respective target values. \n",
    "\n",
    "We choose the second column, the one with index 1, that is, the probabilites of the predicted labels being '1'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
